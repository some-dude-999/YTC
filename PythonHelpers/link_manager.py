#!/usr/bin/env python3
"""
GitHub Pages Link Manager
Automatically generates and updates LINK.txt with GitHub Pages URLs for all HTML files.
"""

import os
import subprocess
from pathlib import Path
from urllib.parse import quote

def get_repo_info():
    """Get repository owner and name from git remote."""
    try:
        # Get remote URL
        result = subprocess.run(
            ['git', 'config', '--get', 'remote.origin.url'],
            capture_output=True,
            text=True,
            check=True
        )
        remote_url = result.stdout.strip()

        # Parse owner and repo from URL
        # Handles HTTPS, SSH, and local proxy formats

        # Handle local proxy format: http://local_proxy@127.0.0.1:port/git/owner/repo
        if '/git/' in remote_url:
            parts = remote_url.split('/git/')[-1].split('/')
            if len(parts) >= 2:
                owner = parts[0]
                repo = parts[1].rstrip('.git')
                return owner, repo

        if 'github.com' in remote_url:
            # Remove .git suffix if present
            remote_url = remote_url.rstrip('.git')

            # Extract owner/repo
            if remote_url.startswith('https://'):
                # https://github.com/owner/repo
                parts = remote_url.split('github.com/')[-1].split('/')
            elif remote_url.startswith('git@'):
                # git@github.com:owner/repo
                parts = remote_url.split('github.com:')[-1].split('/')
            else:
                raise ValueError(f"Unrecognized URL format: {remote_url}")

            if len(parts) >= 2:
                owner = parts[0]
                repo = parts[1]
                return owner, repo

        raise ValueError(f"Could not parse GitHub URL: {remote_url}")

    except subprocess.CalledProcessError as e:
        raise RuntimeError(f"Failed to get git remote: {e}")

def find_html_files(root_dir='.'):
    """Find all .html files in the repository."""
    html_files = []
    root_path = Path(root_dir)

    for html_file in root_path.rglob('*.html'):
        # Skip files in hidden directories or common ignore patterns
        if any(part.startswith('.') for part in html_file.parts):
            continue
        if 'node_modules' in html_file.parts:
            continue

        # Get relative path from root
        rel_path = html_file.relative_to(root_path)
        html_files.append(str(rel_path))

    return sorted(html_files)

def generate_github_pages_url(owner, repo, file_path):
    """Generate GitHub Pages URL for a file."""
    # URL encode the path, preserving slashes
    encoded_path = '/'.join(quote(part, safe='') for part in file_path.split('/'))
    return f"https://{owner}.github.io/{repo}/{encoded_path}"

def read_existing_descriptions(link_file='LINK.txt'):
    """Read existing descriptions from LINK.txt."""
    descriptions = {}

    if not os.path.exists(link_file):
        return descriptions

    try:
        with open(link_file, 'r', encoding='utf-8') as f:
            current_url = None
            for line in f:
                line = line.strip()

                # Skip empty lines and comments
                if not line or line.startswith('#'):
                    continue

                # Check if line contains URL
                if line.startswith('http'):
                    # Split URL and description
                    parts = line.split(' - ', 1)
                    if len(parts) == 2:
                        url = parts[0].strip()
                        desc = parts[1].strip()
                        descriptions[url] = desc
                    else:
                        # Just URL, no description
                        descriptions[line.strip()] = None

    except Exception as e:
        print(f"Warning: Could not read existing LINK.txt: {e}")

    return descriptions

def update_link_file(owner, repo, html_files, link_file='LINK.txt'):
    """Update LINK.txt with all HTML file URLs."""
    existing_descriptions = read_existing_descriptions(link_file)

    # Generate header
    lines = [
        f"# GitHub Pages Links for {owner}/{repo}",
        f"# Auto-generated by PythonHelpers/link_manager.py",
        ""
    ]

    # Generate entries for each HTML file
    for i, html_file in enumerate(html_files, 1):
        url = generate_github_pages_url(owner, repo, html_file)

        # Check if we have an existing description
        description = existing_descriptions.get(url)

        if description:
            # Use existing description
            entry = f"{url} - {description}"
        else:
            # Generate placeholder
            file_name = Path(html_file).stem
            entry = f"{url} - [Add description for {file_name}]"

        lines.append(f"{i}. {Path(html_file).name}")
        lines.append(f"   {entry}")
        lines.append("")

    # Write to file
    try:
        with open(link_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
        print(f"✓ Successfully updated {link_file}")
        return True
    except Exception as e:
        print(f"✗ Error writing to {link_file}: {e}")
        return False

def main():
    """Main function."""
    print("GitHub Pages Link Manager")
    print("=" * 50)

    # Get repository info
    try:
        owner, repo = get_repo_info()
        print(f"Repository: {owner}/{repo}")
    except Exception as e:
        print(f"✗ Error: {e}")
        return 1

    # Find HTML files
    html_files = find_html_files()
    print(f"Found {len(html_files)} HTML file(s)")

    if not html_files:
        print("No HTML files found!")
        return 1

    # Update LINK.txt
    print("\nUpdating LINK.txt...")
    success = update_link_file(owner, repo, html_files)

    if success:
        print("\n✓ Done! Please review LINK.txt and update any placeholder descriptions.")
        return 0
    else:
        return 1

if __name__ == '__main__':
    exit(main())
